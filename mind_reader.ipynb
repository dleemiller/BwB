{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd9fb7f9-30ac-42c6-a5f9-099951af453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import dspy\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # model configuration\n",
    "    # base_model: str = \"ollama_chat/qwen2.5:32b-instruct-q8_0\"\n",
    "    #base_model: str = \"ollama_chat/gemma2:9b-instruct-q8_0\"\n",
    "    # base_model: str = \"ollama_chat/llama3.2:1b-instruct-q8_0\"\n",
    "    # base_model: str = \"ollama_chat/exaone3.5:2.4b-instruct-q8_0\"\n",
    "    # base_model: str = \"ollama_chat/granite3.1-dense:2b-instruct-q8_0\"\n",
    "    # base_model: str = \"ollama_chat/granite3.1-moe:3b-instruct-q8_0\"\n",
    "    # base_model:str = \"ollama_chat/llama3.2:3b-instruct-q8_0\"\n",
    "    #base_model:str = \"ollama_chat/gemma2:27b-instruct-q8_0\"\n",
    "    #base_model: str = \"ollama_chat/phi4:14b-q8_0\"\n",
    "    #base_model: str = \"ollama_chat/command-r:35b-08-2024-q8_0\"\n",
    "    #base_model: str = \"ollama_chat/mistral-small:24b-instruct-2501-q8_0\"\n",
    "    #base_model: str = \"ollama_chat/phi4-mini:3.8b-q8_0\"\n",
    "    temperature: float = 0.7\n",
    "    #base_model: str = \"openrouter/deepseek/deepseek-chat\"\n",
    "    #base_model: str = \"openrouter/deepseek/deepseek-r1\"  # good, slow\n",
    "    #base_model: str = \"openrouter/google/gemini-2.0-flash-001\" # good, fast\n",
    "    #base_model: str = \"openrouter/deepseek/deepseek-r1-distill-qwen-32b\"\n",
    "    #base_model: str = \"openrouter/perplexity/sonar-reasoning\"\n",
    "    #base_model: str = \"openrouter/deepseek/deepseek-r1-distill-llama-70b\"\n",
    "    #base_model: str = \"openrouter/openai/gpt-4o-mini\"\n",
    "    base_model: str = \"openrouter/meta-llama/llama-3.3-70b-instruct\"\n",
    "\n",
    "    # dataset\n",
    "    dataset: str = \"scifact\"\n",
    "    delta_threshold = 0.1\n",
    "\n",
    "    # APIKEY (if using api for teacher)\n",
    "    api_key: str | None = None\n",
    "\n",
    "\n",
    "config = Config(\n",
    "    api_key = os.environ[\"APIKEY\"]\n",
    ")\n",
    "\n",
    "if config.base_model.startswith(\"ollama\"):\n",
    "    # small, locally hosted base model\n",
    "    lm = dspy.LM(config.base_model, api_base='http://localhost:11434', api_key='', temperature=config.temperature, cache=False)\n",
    "else:\n",
    "    lm = dspy.LM(config.base_model, api_key=config.api_key, temperature=config.temperature, cache=False)\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "966b1aaa-fe19-4a8b-b752-eb4fa6d9a300",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIND_READER = \"\"\"\n",
    "# Query Expansion Narrative Generation\n",
    "\n",
    "## Task Overview\n",
    "You are tasked with creating a first-person narrative that recreates the thought process behind expanding a search query.\n",
    "This narrative should explain how you would move from the original query to the provided augmented query using the specific technique indicated in the instruction.\n",
    "\n",
    "## Input Format\n",
    "You will receive three components:\n",
    "\n",
    "1. **Expansion Technique**: The original instruction of the technique to use (e.g., \"expand\", \"graph\", \"headline\", etc.)\n",
    "\n",
    "2. **Structured Data in Markdown Format**:\n",
    "   ```markdown\n",
    "   ## Query\n",
    "   [original query text]\n",
    "   \n",
    "   ## Existing Reasoning\n",
    "   [brief summary of reasoning]\n",
    "   \n",
    "   ## Task Outcomes\n",
    "   ### Query Goals\n",
    "   - [goal 1]\n",
    "   - [goal 2]\n",
    "   \n",
    "   ### [Other Fields Based on Technique]\n",
    "   - [item 1]\n",
    "   - [item 2]\n",
    "   \n",
    "   ## Augmented Query\n",
    "   [final expanded query]\n",
    "   ```\n",
    "\n",
    "3. **Technique Description**: A separate description of the specific technique to apply (provided externally)\n",
    "\n",
    "## Format\n",
    "\n",
    "1. **Initiate the thinking***: Start each thought process as brainstorming about the intent for what the user is looking for in the query. For example, something like:\n",
    "\"The user is search a BM25 index for '<original query>'. I need to uncover optimal search terms to use with that algorithm by\n",
    "determining the intent of their search. The user seems to be looking for...\"\n",
    "\n",
    "2. **Playing the Mind Game**: First, state and very briefly explain the mind game you are playing (expansion technique). Then,\n",
    "walk through steps in the mind game as though you are uncovering search terms as you think more deeply about the query.\n",
    "\n",
    "3. **Augmented query terms**: After a detailed first person role play of the mind game technique, conclude with something like:\n",
    "\"Therefore, the optimal BM25 search terms are <search terms from the augmented query>\"\n",
    "\n",
    "These augmented query terms should include all important words from the augmented query, but written as list of terms without\n",
    "unnecessary things like punctuation or stopwords, OR/AND operators etc. Remember, this is a BM25 index and those terms have a\n",
    "high document frequency or get removed in the tokenization process anyway. However, be sure to include ALL **important** terms\n",
    "from the augmented query and nothing more.\n",
    "\n",
    "Remember, ensure that your thought process exemplifies the detailed steps of the expansion technique, as though you were\n",
    "using each step in the technique to uncover hidden and more rich search terms.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c54afc19-d14d-43be-99bf-cbed4260252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "\n",
    "class MindReader(dspy.Signature):\n",
    "    __doc__ = MIND_READER\n",
    "    structured_data: str = dspy.InputField(desc=\"Query augmentation process data\")\n",
    "    expansion_technique: str = dspy.InputField(desc=\"Mind game used as technique for query expansion\")\n",
    "    thought_process: str = dspy.OutputField(desc=\"First person role-playing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35ebe7b8-c31e-465d-b20e-74abaa208110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"generator_docstrings.json\", \"r\") as fh:\n",
    "    instructions = json.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8caa43d-c89b-4005-ba4e-5a13108a5ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import json\n",
    "import sys\n",
    "\n",
    "def format_json_to_markdown(json_data):\n",
    "    # Create a copy to avoid modifying the original\n",
    "    data = json_data.copy()\n",
    "    \n",
    "    # Extract key fields before removing them\n",
    "    query = data.pop('query', 'No query provided')\n",
    "    reasoning = data.pop('reasoning', 'No reasoning provided')\n",
    "    augmented_query = data.pop('augmented_query', 'No augmented query provided')\n",
    "    instruction = data.pop('instruction', 'No instruction provided')\n",
    "    \n",
    "    # Remove performance/accuracy related fields\n",
    "    metrics_fields = ['ndcg', 'map', 'recall', 'precision', 'delta']\n",
    "    for field in metrics_fields:\n",
    "        if field in data:\n",
    "            del data[field]\n",
    "    \n",
    "    # Remove dataset, generator, temperature info\n",
    "    other_fields = ['dataset', 'generator_model', 'temperature', 'query_id']\n",
    "    for field in list(data.keys()):\n",
    "        if field in other_fields:\n",
    "            del data[field]\n",
    "    \n",
    "    # Build markdown output\n",
    "    markdown = []\n",
    "    \n",
    "    # Add query section\n",
    "    markdown.append(\"## Query\")\n",
    "    markdown.append(f\"{query}\")\n",
    "    markdown.append(\"\")\n",
    "    \n",
    "    # Add reasoning section\n",
    "    markdown.append(\"## Existing Reasoning\")\n",
    "    markdown.append(f\"{reasoning}\")\n",
    "    markdown.append(\"\")\n",
    "    \n",
    "    # Add task outcomes section for remaining fields\n",
    "    markdown.append(\"## Task Outcomes\")\n",
    "    \n",
    "    for key, value in data.items():\n",
    "        formatted_key = key.replace('_', ' ').title()\n",
    "        markdown.append(f\"### {formatted_key}\")\n",
    "        \n",
    "        if isinstance(value, list):\n",
    "            for item in value:\n",
    "                markdown.append(f\"- {item}\")\n",
    "        else:\n",
    "            markdown.append(f\"{value}\")\n",
    "        \n",
    "        markdown.append(\"\")\n",
    "    \n",
    "    # Add augmented query section\n",
    "    markdown.append(\"## Augmented Query\")\n",
    "    markdown.append(f\"{augmented_query}\")\n",
    "    \n",
    "    return \"\\n\".join(markdown), instruction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36a37b7e-39fb-4167-b83f-9aa87cdac524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def process_benchmark(input_dir, output_dir, reprocess=True):\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get all JSON files in the input directory\n",
    "    json_files = glob.glob(os.path.join(input_dir, \"*.json\"))\n",
    "    \n",
    "    # Track statistics\n",
    "    total_files = len(json_files)\n",
    "    processed_files = 0\n",
    "    skipped_files = 0\n",
    "    \n",
    "    # Process each file\n",
    "    for file_path in json_files:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        output_path = os.path.join(output_dir, file_name)\n",
    "        \n",
    "        # Check if file already exists and skip if reprocess is False\n",
    "        if os.path.exists(output_path) and not reprocess:\n",
    "            print(f\"Skipping {file_path} (already exists in output directory)\")\n",
    "            skipped_files += 1\n",
    "            continue\n",
    "        \n",
    "        print(f\"Processing {file_path}...\")\n",
    "        \n",
    "        # Read the JSON file\n",
    "        with open(file_path, 'r') as file:\n",
    "            json_data = json.load(file)\n",
    "        \n",
    "        # Convert to markdown and get the instruction\n",
    "        markdown_content, instruction = format_json_to_markdown(json_data)\n",
    "        \n",
    "        # Check if instruction exists in instructions dictionary\n",
    "        if instruction not in instructions:\n",
    "            print(f\"Warning: Instruction '{instruction}' not found in instructions dictionary. Skipping {file_path}\")\n",
    "            skipped_files += 1\n",
    "            continue\n",
    "        \n",
    "        # Generate thought process using dspy\n",
    "        try:\n",
    "            gen = dspy.Predict(MindReader)  # Assuming MindReader is defined elsewhere\n",
    "            pred = gen(\n",
    "                structured_data=markdown_content,\n",
    "                expansion_technique=instructions[instruction]\n",
    "            )\n",
    "            \n",
    "            # Add thought process to the original JSON data\n",
    "            print(pred)\n",
    "            json_data[\"thought_process\"] = pred.thought_process\n",
    "            json_data[\"though_generator\"] = config.base_model\n",
    "            \n",
    "            # Save the augmented JSON to the output directory\n",
    "            with open(output_path, 'w') as outfile:\n",
    "                json.dump(json_data, outfile, indent=2)\n",
    "                \n",
    "            print(f\"Saved augmented JSON to {output_path}\")\n",
    "            processed_files += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "            skipped_files += 1\n",
    "    \n",
    "    print(f\"Processing complete:\")\n",
    "    print(f\"- Total files: {total_files}\")\n",
    "    print(f\"- Processed files: {processed_files}\")\n",
    "    print(f\"- Skipped files: {skipped_files}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44895bc7-cf5b-43df-aed3-59538cd7ddcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data/scifact/1213.json...\n",
      "Prediction(\n",
      "    thought_process='The user is searching a BM25 index for \\'The deregulated and prolonged activation of monocytes has deleterious effects in inflammatory diseases\\'. I need to uncover optimal search terms to use with that algorithm by determining the intent of their search. The user seems to be looking for information on how prolonged monocyte activation affects inflammatory diseases and potentially seeking treatment strategies or consequences of this activation.\\n\\nTo expand this query, I\\'ll be playing a mind game called \"Query Expansion via User Intent and Journey Mapping\". This technique involves analyzing how different users, with varying levels of expertise, would seek information about prolonged monocyte activation in inflammatory diseases throughout their learning journey. The goal is to extract intent-revealing terms and vocabulary that reflect how real users describe their needs at different stages.\\n\\nFirst, I\\'ll create user personas representing different levels of expertise: novice, intermediate, advanced, and expert. Each persona has unique goals, motivations, prior knowledge, and vocabulary. For instance, a novice might start with general terms like \\'inflammation\\', \\'monocytes\\', and \\'immune system\\', while an expert would use more specialized terms like \\'Treg cell dysfunction\\', \\'metabolic reprogramming\\', and \\'monocyte heterogeneity\\'.\\n\\nAs I map the information journey of these personas, I notice that their search vocabulary evolves significantly. At the entry point, users might search for symptoms and general information about inflammation. As they progress to intermediate stages, they seek mechanisms of monocyte activation and its role in disease progression. Advanced users delve into molecular pathways, cell signaling, and therapeutic targets related to sustained monocyte activation. Experts, finally, focus on clinical implications, patient stratification, and personalized medicine approaches targeting prolonged monocyte activity.\\n\\nConsidering various usage scenarios, such as a patient seeking self-education, a medical student studying immunology, a researcher looking for specific data, or a clinician making treatment decisions, helps in identifying context-specific terms. These scenarios highlight the importance of understanding the user\\'s intent behind their search, whether it\\'s for general knowledge, academic purposes, research, or clinical decision-making.\\n\\nBy analyzing the intent terms extracted from all personas and scenarios, I can identify high-value terms that reflect the users\\' actual information needs. Terms like \\'causes of inflammation\\', \\'impact of prolonged monocytes\\', \\'inflammatory diseases\\', \\'monocyte activation mechanisms\\', \\'therapeutic targets for chronic inflammation\\', and \\'clinical implications of monocyte dysregulation\\' are crucial for expanding the original query.\\n\\nTherefore, the optimal BM25 search terms are prolonged monocyte activation consequences inflammatory diseases treatment strategies, which include terms like monocyte activation, inflammatory diseases, treatment strategies, consequences, and prolonged activation. These terms encompass the essential information needs of users across different expertise levels and usage scenarios, ensuring a comprehensive search query that captures the complexity of the topic.'\n",
      ")\n",
      "Saved augmented JSON to data/thinking/scifact/1213.json\n",
      "Processing data/scifact/770.json...\n",
      "Prediction(\n",
      "    thought_process=\"The user is searching a BM25 index for 'Metastatic colorectal cancer treated with a single agent fluoropyrimidines resulted in reduced efficacy and lower quality of life when compared with oxaliplatin-based chemotherapy in elderly patients.' I need to uncover optimal search terms to use with that algorithm by determining the intent of their search. The user seems to be looking for information on the treatment of metastatic colorectal cancer, specifically comparing the efficacy and quality of life outcomes of single-agent fluoropyrimidines versus oxaliplatin-based chemotherapy in elderly patients.\\n\\nTo expand this query, I will play the mind game of Query Expansion via Conceptual Graph Traversal. This technique involves analyzing the query, constructing a conceptual graph, selecting expansion paths, extracting terms, and filtering them based on relevance, distinctiveness, specificity, and searchability.\\n\\nFirst, I analyze the query to identify core concepts and entities: metastatic colorectal cancer, fluoropyrimidines, oxaliplatin, and elderly patients. The search intent is informational, seeking to understand the comparison between two treatment approaches. Implicit assumptions include the importance of efficacy and quality of life in treatment decisions.\\n\\nNext, I construct a conceptual graph with core nodes (metastatic colorectal cancer, fluoropyrimidines, oxaliplatin, elderly patients), first-order connections (treatment outcomes, patient characteristics), second-order connections (related treatments, age-related health issues), and domain contexts (oncology, geriatrics).\\n\\nI then select promising expansion paths, including synonymous terms (e.g., capecitabine for fluoropyrimidines), hierarchical terms (e.g., colorectal cancer subtypes), compositional terms (e.g., components of oxaliplatin-based chemotherapy), contextual terms (e.g., settings where treatment is administered), and domain-specific terminology (e.g., progression-free survival).\\n\\nFor each expansion path, I extract high-value terms: capecitabine, irinotecan, levamisole, age-related frailty, comorbidities, progression-free survival, overall survival, and quality of life measures. I filter these terms based on their relevance, distinctiveness, specificity, and searchability, ensuring they maintain the original search intent.\\n\\nTherefore, the optimal BM25 search terms are: metastatic colorectal cancer, capecitabine, oxaliplatin, elderly, age-related frailty, comorbidities, progression-free survival, overall survival, quality of life.\"\n",
      ")\n",
      "Saved augmented JSON to data/thinking/scifact/770.json\n",
      "Processing data/scifact/628.json...\n",
      "Prediction(\n",
      "    thought_process='The user is searching a BM25 index for \"Infection of human T-cell lymphotropic virus type 1 is most frequent in individuals of African origin.\" I need to uncover optimal search terms to use with that algorithm by determining the intent of their search. The user seems to be looking for information on the prevalence and distribution of HTLV-1 infection, specifically in individuals of African origin.\\n\\nTo expand this query, I will be playing the mind game of \"Query Expansion via Conceptual Graph Traversal.\" This technique involves constructing a conceptual graph and traversing it to find high-value expansion terms. I will start by identifying core concepts and entities in the original query, such as \"HTLV-1,\" \"infection,\" \"African origin,\" and \"prevalence.\" The search intent appears to be informational, as the user is seeking data on the epidemiology of HTLV-1 in Africa.\\n\\nNext, I will build a conceptual graph with core nodes, first-order connections, second-order connections, and domain contexts. The core nodes will include \"HTLV-1,\" \"infection,\" and \"African origin.\" First-order connections will involve properties and attributes of these concepts, such as \"prevalence,\" \"epidemiology,\" and \"geographic distribution.\" Second-order connections will include related concepts like \"risk factors,\" \"transmission patterns,\" and \"endemic diseases.\" Domain contexts will be specialized fields such as \"virology,\" \"epidemiology,\" and \"public health.\"\\n\\nI will then identify promising directions for query expansion, including synonymous terms, hierarchical terms, compositional terms, contextual terms, and domain-specific terminology. For example, synonymous terms might include \"HTLV-1 prevalence\" and \"HTLV-1 epidemiology.\" Hierarchical terms could involve broader concepts like \"retroviral infections\" or narrower concepts like \"HTLV-1 subtype distribution.\" Compositional terms might include \"HTLV-1 transmission\" and \"HTLV-1 risk factors.\" Contextual terms could involve settings like \"Africa\" or \"sub-Saharan Africa,\" while domain-specific terminology might include \"seroprevalence\" and \"molecular epidemiology.\"\\n\\nAs I traverse the conceptual graph, I will extract high-value terms from each expansion path and filter them based on relevance, distinctiveness, specificity, and searchability. This process will yield a set of high-quality expansion terms that maintain the original search intent while providing valuable alternative pathways for retrieval.\\n\\nTherefore, the optimal BM25 search terms are HTLV-1, prevalence, epidemiology, Africa, African origin, risk factors, transmission, retroviral, infection, seroprevalence, molecular epidemiology, geographic distribution, endemic diseases, public health, virology.'\n",
      ")\n",
      "Saved augmented JSON to data/thinking/scifact/628.json\n",
      "Processing data/scifact/475.json...\n",
      "Prediction(\n",
      "    thought_process='The user is searching a BM25 index for \\'Glycolysis is one of the primary glycometabolic pathways in cells.\\' I need to uncover optimal search terms to use with that algorithm by determining the intent of their search. The user seems to be looking for a comprehensive understanding of glycolysis, including its biochemical mechanisms, regulatory controls, and diverse functional implications across different cellular processes.\\n\\nTo achieve this, I will engage in a mind game called Conceptual Territory Mapping. This technique involves creating a detailed map of the conceptual territory surrounding the topic of glycolysis. The process begins with identifying the central coordinates of the query, which in this case is the glycolytic pathway itself. I establish the cardinal directions by considering the practical and theoretical aspects of glycolysis, as well as its technical and social implications. The scale for exploration is set to be granular, focusing on the intricacies of the biochemical mechanisms and regulatory controls.\\n\\nNext, I chart the boundaries of the query\\'s immediate territory, which includes neighboring domains such as gluconeogenesis, the pentose phosphate pathway, aerobic respiration, anaerobic metabolism, and cancer metabolism. I note areas where these territories overlap or connect, such as the regulation of glycolysis in cancer cells.\\n\\nI then identify major landmarks within the territory, including key enzymes like hexokinase, phosphofructokinase, and pyruvate kinase, as well as processes like glycogen synthesis, lactate fermentation, and the Warburg effect. These landmarks serve as essential resources for understanding the complexities of glycolysis.\\n\\nThe detail mapping stage involves filling in specialized features within each region, labeling them with precise, domain-specific terminology. I note historical sites, such as the discovery of the glycolytic pathway, and emerging developments, like the role of glycolysis in cancer metabolism. I also identify \"settlements\" or communities of practice, such as the field of cancer research, and \"routes\" or methodologies, like the use of enzyme kinetics to study glycolysis.\\n\\nFinally, I create a cartographic legend, which is a lexicon of the most important terms on my map. I classify them by region and significance, selecting terms that would help a traveler navigate this knowledge landscape. These terms include glycolysis, biochemical mechanisms, regulation, metabolic pathways, cellular energy production, enzyme kinetics, and applications in disease.\\n\\nTherefore, the optimal BM25 search terms are glycolysis, biochemical mechanisms, regulation, metabolic pathways, cellular energy production, enzyme kinetics, applications, disease.'\n",
      ")\n",
      "Saved augmented JSON to data/thinking/scifact/475.json\n",
      "Processing data/scifact/421.json...\n",
      "Prediction(\n",
      "    thought_process='The user is searching a BM25 index for \"Flexible molecules experience greater steric hindrance in the tumor microenviroment than rigid molecules.\" I need to uncover optimal search terms to use with that algorithm by determining the intent of their search. The user seems to be looking for information on how the flexibility of molecules affects their interaction with the tumor microenvironment, specifically in terms of steric hindrance.\\n\\nTo expand this query, I will play the mind game of Conceptual Territory Mapping. This technique involves creating a detailed map of the conceptual territory surrounding the topic, which includes identifying central coordinates, exploring boundaries, identifying landmarks, detailing features, and creating a cartographic legend.\\n\\nFirst, I identify the central point of the query: the impact of molecular flexibility on steric hindrance in the tumor microenvironment. The cardinal directions for exploration include the practical/theoretical aspects of molecular structure, the technical/social aspects of drug delivery and efficacy, and the biological/physical aspects of the tumor microenvironment.\\n\\nAs I explore the boundaries of this territory, I note neighboring domains such as drug delivery systems, cancer cell signaling, tumor immunology, biomaterial properties, and pharmacokinetics. I also identify areas where these territories overlap, such as the role of the extracellular matrix in drug delivery and the impact of molecular flexibility on cell membrane interactions.\\n\\nI then identify key landmarks in this territory, including diffusion barriers, cell membrane receptors, extracellular matrix, angiogenesis, and nanoparticle drug delivery. These landmarks help me understand the complex interactions between molecules, cells, and the tumor microenvironment.\\n\\nNext, I detail the specialized features within each region, using precise, domain-specific terminology. I note historical sites, such as the development of drug delivery systems, and emerging developments, such as the use of nanoparticles for targeted therapy.\\n\\nFinally, I create a cartographic legend, classifying the most important terms by region and significance. The navigation terms that emerge from this process include \"steric hindrance,\" \"tumor microenvironment,\" \"flexible molecules,\" \"rigid molecules,\" \"drug delivery,\" \"cell membrane,\" \"extracellular matrix,\" and \"nanoparticles.\"\\n\\nTherefore, the optimal BM25 search terms are: steric hindrance, tumor microenvironment, flexible molecules, rigid molecules, drug delivery, cell membrane, extracellular matrix, nanoparticles, molecular flexibility, diffusion barriers, angiogenesis, and targeted therapy.'\n",
      ")\n",
      "Saved augmented JSON to data/thinking/scifact/421.json\n",
      "Processing data/scifact/1278.json...\n",
      "Prediction(\n",
      "    thought_process='The user is searching a BM25 index for \"The treatment of cancer patients with co-IR blockade does not cause any adverse autoimmune events.\" I need to uncover optimal search terms to use with that algorithm by determining the intent of their search. The user seems to be looking for information on the safety of cancer treatment using co-inhibitory blockade, specifically regarding the absence of adverse autoimmune events.\\n\\nTo expand this query, I will be playing the mind game of \"Query Expansion via Conceptual Graph Traversal.\" This technique involves analyzing the query, constructing a conceptual graph, selecting expansion paths, and extracting high-value terms. \\n\\nFirst, I analyze the query to identify core concepts: cancer treatment, co-IR blockade, and autoimmune events. The search intent is informational, seeking to understand the safety profile of a specific medical intervention. \\n\\nNext, I construct a conceptual graph with core nodes (cancer treatment, co-IR blockade, autoimmune events), first-order connections (immunotherapy, side effects), and second-order connections (checkpoints, immunotherapy safety). The domain context is oncology and immunology.\\n\\nThen, I select promising expansion paths: \\n- Synonymous terms (e.g., \"cancer therapy\" instead of \"cancer treatment\"),\\n- Hierarchical terms (e.g., \"immunotherapy\" as a broader concept),\\n- Compositional terms (e.g., \"co-inhibitory receptors\" as components),\\n- Contextual terms (e.g., \"clinical trials\" as settings),\\n- Domain-specific terminology (e.g., \"PD-1/PD-L1 blockade\" as field-specific variants).\\n\\nFor each expansion path, I extract high-value terms: \\n- Synonyms: cancer therapy, oncology treatment\\n- Hierarchical: immunotherapy, checkpoint inhibition\\n- Compositional: co-inhibitory receptors, immune checkpoints\\n- Contextual: clinical trials, cancer research\\n- Domain-specific: PD-1/PD-L1 blockade, CTLA-4 blockade\\n\\nAfter filtering these terms based on relevance, distinctiveness, specificity, and searchability, I obtain a set of expanded terms that maintain the original search intent while providing valuable alternative pathways for retrieval.\\n\\nTherefore, the optimal BM25 search terms are cancer treatment, immunotherapy, side effects, co-inhibitory blockade, autoimmune events, checkpoints, PD-1, PD-L1, CTLA-4, oncology.'\n",
      ")\n",
      "Saved augmented JSON to data/thinking/scifact/1278.json\n",
      "Processing data/scifact/659.json...\n"
     ]
    }
   ],
   "source": [
    "beir = \"scifact\"\n",
    "INPUT_FOLDER = f\"data/{beir}\"\n",
    "OUTPUT_FOLDER = f\"data/thinking/{beir}\"\n",
    "\n",
    "process_benchmark(INPUT_FOLDER, OUTPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357bf8dd-b941-49f5-99e9-a028c8064955",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open(\"data/scifact/1014.json\", 'r') as file:\n",
    "#     json_data = json.load(file)\n",
    "\n",
    "# # Convert to markdown and print\n",
    "# markdown_content, instruction = format_json_to_markdown(json_data)\n",
    "\n",
    "\n",
    "# gen = dspy.Predict(MindReader)\n",
    "# pred = gen(\n",
    "#     structured_data = markdown_content,\n",
    "#     expansion_technique = instructions[instruction]\n",
    "# )\n",
    "\n",
    "# print(pred.thought_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e7cb63-23f1-46f6-acee-7785a26d20c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
